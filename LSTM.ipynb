{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as np\n",
    "import pickle\n",
    "df = np.read_csv('mbti_1.csv')\n",
    "\n",
    "# populating types array\n",
    "labels = []\n",
    "for _type in df['type']:\n",
    "     labels.append(_type)\n",
    "\n",
    "with open('processed_posts.pkl', 'rb') as pf:\n",
    "    posts = pickle.load(pf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "countedPosts = [len(post) for post in posts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsByPost = []\n",
    "postsByPost = []\n",
    "\n",
    "for i in range(0, len(posts)):\n",
    "    for post in posts[i]:\n",
    "        labelsByPost.append(labels[i])\n",
    "        postsByPost.append(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing data\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "max_features = 10000\n",
    "max_len = 25\n",
    "batch_size = 32\n",
    "max_words = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 75302 unique tokens\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(postsByPost)\n",
    "sequences = tokenizer.texts_to_sequences(postsByPost)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sequence.pad_sequences(sequences, maxlen=max_len)\n",
    "label_dictionary = {\n",
    "    'I': 0,\n",
    "    'E': 1\n",
    "}\n",
    "\n",
    "labels = [label_dictionary[label[0]] for label in labelsByPost]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "labels = np.asarray(labels).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (383869, 25)\n",
      "Shape of label tensor: (383869,)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Can't do this because it will lose track of the users posts\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the test labels and test data\n",
    "# from the rest, so we can keep track of users accordingly\n",
    "# Getting the first 800 users for testing\n",
    "# Getting users 800-1000 for validating\n",
    "testUsers = 35431\n",
    "valUsers = 8901\n",
    "\n",
    "x_test = data[:35431]\n",
    "y_test = labels[:35431]\n",
    "\n",
    "x_val = data[35431:35431+8901]\n",
    "y_val = labels[35431:35431+8901]\n",
    "\n",
    "data = data[35431+8901:]\n",
    "labels = labels[35431+8901:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_boundaries = countedPosts[:800]\n",
    "val_boundaries = countedPosts[800:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_labels = []\n",
    "e_data = []\n",
    "i_labels = []\n",
    "i_data = []\n",
    "\n",
    "for i in range(0, len(labels)):\n",
    "    if (labels[i] == 0):\n",
    "        i_labels.append(labels[i])\n",
    "        i_data.append(data[i])\n",
    "    if (labels[i] == 1):\n",
    "        e_labels.append(labels[i])\n",
    "        e_data.append(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mix_labels = e_labels + i_labels[:len(e_labels)]\n",
    "mix_data = e_data + i_data[:len(e_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mix_labels = np.asarray(mix_labels).astype('float32')\n",
    "mix_data = np.array(mix_data).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(mix_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "mix_data = mix_data[indices]\n",
    "mix_labels = mix_labels[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train = mix_data\n",
    "y_train = mix_labels"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('glove.6B.50d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "embedding_dim = 50\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "\n",
    "class True_eval(Callback):\n",
    "    def __init__(self, validation_data):\n",
    "        self.validation_data = validation_data\n",
    "        self.i_accuracy = []\n",
    "        self.e_accuracy = []\n",
    "        self.user_accuracy = []\n",
    "    \n",
    "    def ie_real_acc(self, prediction):\n",
    "        y_true = self.validation_data[1]\n",
    "        i_acc = 0\n",
    "        i_total = 0\n",
    "        e_acc = 0\n",
    "        e_total = 0\n",
    "        for i in range(0, len(prediction)):\n",
    "            if (y_true[i] == 0):\n",
    "                if (prediction[i].round() == y_true[i]):\n",
    "                    i_acc += 1\n",
    "                i_total += 1\n",
    "            else:\n",
    "                if (prediction[i].round() == y_true[i]):\n",
    "                    e_acc += 1\n",
    "                e_total += 1\n",
    "        return (i_acc/i_total), (e_acc/e_total)\n",
    "    \n",
    "    def user_acc(self, prediction):\n",
    "        y_true = self.validation_data[1]\n",
    "        usersCorrect = []\n",
    "        start = 0\n",
    "        for length in val_boundaries:\n",
    "            correct = 0\n",
    "            for i in range(0, length):\n",
    "                if (y_true[i+start] == prediction[i+start].round()):\n",
    "                    correct += 1\n",
    "            \n",
    "            usersCorrect.append(round(correct/length))\n",
    "            start += length\n",
    "            \n",
    "        print (usersCorrect)\n",
    "        return sum(usersCorrect)/len(usersCorrect)\n",
    "                \n",
    "        \n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x_val = self.validation_data[0]\n",
    "        y_pred = self.model.predict(x_val)\n",
    "        i_real_acc, e_real_acc = self.ie_real_acc(y_pred)\n",
    "        user_accuracy = self.user_acc(y_pred)\n",
    "        print (\"I Acc: %f\" % i_real_acc)\n",
    "        print (\"E Acc: %f\" % e_real_acc)\n",
    "        print (\"User Acc: %f\" % user_accuracy)\n",
    "        self.i_accuracy.append(i_real_acc)\n",
    "        self.e_accuracy.append(e_real_acc)\n",
    "        self.user_accuracy.append(user_accuracy)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('glove.6B.50d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "embedding_dim = 50\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.regularizers import l1_l2\n",
    "from keras.layers import Embedding, Dropout, LSTM, Dense\n",
    "\n",
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(Embedding(max_features, 128, input_length=max_len))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    #model.layers[0].set_weights([embedding_matrix])\n",
    "    #model.layers[0].trainable = False\n",
    "\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_18 to have shape (1,) but got array with shape (25,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-cdad1154b7d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                         callbacks=[true_metrics])\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    953\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    956\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    790\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 792\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    134\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    137\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_18 to have shape (1,) but got array with shape (25,)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "model = build_model()\n",
    "with tf.device('/gpu:0'):\n",
    "    true_metrics = True_eval((x_val, y_val))\n",
    "    history = model.fit(x_train, x_train,\n",
    "                        validation_data=(x_val, y_val),\n",
    "                        epochs=100, batch_size=256,\n",
    "                        callbacks=[true_metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_dict = history.history\n",
    "train_loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "train_acc = history_dict['categorical_accuracy']\n",
    "val_acc   = history_dict['val_categorical_accuracy']\n",
    "\n",
    "epochs = range(1, len(history_dict['categorical_accuracy']) + 1)\n",
    "\n",
    "plt.plot(epochs, train_loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "plt.plot(epochs, train_acc, 'bo', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation Accuracy')\n",
    "plt.title('Training and validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(epochs, true_metrics.i_accuracy, 'b', label='I Accuracy')\n",
    "plt.plot(epochs, true_metrics.e_accuracy, 'y', label='E Accuracy')\n",
    "plt.title('Letter Accuracies')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "URE18",
   "language": "python",
   "name": "ure18"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

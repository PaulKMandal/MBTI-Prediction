{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('embedding_labels.pkl', 'rb') as lf:\n",
    "    labels = pickle.load(lf)\n",
    "\n",
    "with open('embedding_posts.pkl', 'rb') as pf:\n",
    "    posts = pickle.load(pf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "388065"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing data\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "max_features = 10000\n",
    "max_len = 50\n",
    "batch_size = 32\n",
    "train_samples = 329885 # Roughly about 70% of Data\n",
    "max_words = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 116166 unique tokens\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(posts)\n",
    "sequences = tokenizer.texts_to_sequences(posts)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sequence.pad_sequences(sequences, maxlen=max_len)\n",
    "label_dictionary = {\n",
    "    'I': 0,\n",
    "    'E': 1\n",
    "}\n",
    "\n",
    "labels = [label_dictionary[label[:1]] for label in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "labels = np.asarray(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (388065, 50)\n",
      "Shape of label tensor: (388065,)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = data[train_samples:]\n",
    "y_test = labels[train_samples:]\n",
    "\n",
    "x_train = data[:train_samples]\n",
    "y_train = labels[:train_samples]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('glove.6B.50d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "embedding_dim = 50\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 90733, 0: 297332})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = {\n",
    "    0:1.0,\n",
    "    1:3.27\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 280402 samples, validate on 49483 samples\n",
      "Epoch 1/40\n",
      "280402/280402 [==============================] - 203s 725us/step - loss: 1.0527 - acc: 0.5384 - val_loss: 1.0441 - val_acc: 0.6392\n",
      "Epoch 2/40\n",
      "280402/280402 [==============================] - 201s 717us/step - loss: 1.0394 - acc: 0.5616 - val_loss: 1.0442 - val_acc: 0.4735\n",
      "Epoch 3/40\n",
      "280402/280402 [==============================] - 200s 715us/step - loss: 1.0282 - acc: 0.5702 - val_loss: 1.0544 - val_acc: 0.6228\n",
      "Epoch 4/40\n",
      "280402/280402 [==============================] - 200s 713us/step - loss: 1.0152 - acc: 0.5856 - val_loss: 1.0624 - val_acc: 0.4542\n",
      "Epoch 5/40\n",
      "280402/280402 [==============================] - 201s 718us/step - loss: 1.0021 - acc: 0.5993 - val_loss: 1.0582 - val_acc: 0.5420\n",
      "Epoch 6/40\n",
      "280402/280402 [==============================] - 200s 714us/step - loss: 0.9904 - acc: 0.6073 - val_loss: 1.1046 - val_acc: 0.4508\n",
      "Epoch 7/40\n",
      "280402/280402 [==============================] - 200s 714us/step - loss: 0.9795 - acc: 0.6171 - val_loss: 1.0897 - val_acc: 0.4735\n",
      "Epoch 8/40\n",
      "280402/280402 [==============================] - 201s 719us/step - loss: 0.9684 - acc: 0.6261 - val_loss: 1.0774 - val_acc: 0.5680\n",
      "Epoch 9/40\n",
      "280402/280402 [==============================] - 201s 716us/step - loss: 0.9586 - acc: 0.6320 - val_loss: 1.0975 - val_acc: 0.4914\n",
      "Epoch 10/40\n",
      "280402/280402 [==============================] - 202s 719us/step - loss: 0.9475 - acc: 0.6383 - val_loss: 1.0933 - val_acc: 0.5711\n",
      "Epoch 11/40\n",
      "280402/280402 [==============================] - 201s 718us/step - loss: 0.9358 - acc: 0.6466 - val_loss: 1.1672 - val_acc: 0.4222\n",
      "Epoch 12/40\n",
      "280402/280402 [==============================] - 201s 716us/step - loss: 0.9238 - acc: 0.6525 - val_loss: 1.1294 - val_acc: 0.5196\n",
      "Epoch 13/40\n",
      "280402/280402 [==============================] - 201s 717us/step - loss: 0.9112 - acc: 0.6614 - val_loss: 1.1408 - val_acc: 0.5028\n",
      "Epoch 14/40\n",
      "280402/280402 [==============================] - 201s 718us/step - loss: 0.8987 - acc: 0.6686 - val_loss: 1.1776 - val_acc: 0.5309\n",
      "Epoch 15/40\n",
      "280402/280402 [==============================] - 201s 719us/step - loss: 0.8855 - acc: 0.6761 - val_loss: 1.1269 - val_acc: 0.5666\n",
      "Epoch 16/40\n",
      "280402/280402 [==============================] - 202s 720us/step - loss: 0.8712 - acc: 0.6843 - val_loss: 1.2145 - val_acc: 0.5167\n",
      "Epoch 17/40\n",
      "280402/280402 [==============================] - 201s 716us/step - loss: 0.8575 - acc: 0.6914 - val_loss: 1.2423 - val_acc: 0.4482\n",
      "Epoch 18/40\n",
      "280402/280402 [==============================] - 201s 716us/step - loss: 0.8451 - acc: 0.6970 - val_loss: 1.2379 - val_acc: 0.5208\n",
      "Epoch 19/40\n",
      "280402/280402 [==============================] - 201s 717us/step - loss: 0.8313 - acc: 0.7045 - val_loss: 1.2475 - val_acc: 0.5701\n",
      "Epoch 20/40\n",
      "280402/280402 [==============================] - 201s 718us/step - loss: 0.8179 - acc: 0.7117 - val_loss: 1.2447 - val_acc: 0.4921\n",
      "Epoch 21/40\n",
      "280402/280402 [==============================] - 201s 718us/step - loss: 0.8058 - acc: 0.7170 - val_loss: 1.3239 - val_acc: 0.4554\n",
      "Epoch 22/40\n",
      "280402/280402 [==============================] - 201s 717us/step - loss: 0.7916 - acc: 0.7235 - val_loss: 1.3324 - val_acc: 0.5815\n",
      "Epoch 23/40\n",
      "280402/280402 [==============================] - 202s 720us/step - loss: 0.7790 - acc: 0.7300 - val_loss: 1.3360 - val_acc: 0.5727\n",
      "Epoch 24/40\n",
      "280402/280402 [==============================] - 200s 715us/step - loss: 0.7656 - acc: 0.7359 - val_loss: 1.3543 - val_acc: 0.5193\n",
      "Epoch 25/40\n",
      "280402/280402 [==============================] - 201s 718us/step - loss: 0.7513 - acc: 0.7417 - val_loss: 1.3099 - val_acc: 0.5332\n",
      "Epoch 26/40\n",
      "280402/280402 [==============================] - 201s 717us/step - loss: 0.7393 - acc: 0.7472 - val_loss: 1.3627 - val_acc: 0.5410\n",
      "Epoch 27/40\n",
      "225024/280402 [=======================>......] - ETA: 38s - loss: 0.7198 - acc: 0.7557"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.regularizers import l1_l2\n",
    "from keras.layers import Embedding, Dropout, LSTM, Dense\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(Embedding(max_features, max_len))\n",
    "model.add(LSTM(16, return_sequences=True))\n",
    "model.add(LSTM(16))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# model.layers[0].set_weights([embedding_matrix])\n",
    "# model.layers[0].trainable = False\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=40, batch_size=256, class_weight=class_weights, validation_split=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "URE18",
   "language": "python",
   "name": "ure18"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

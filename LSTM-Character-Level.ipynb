{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as np\n",
    "import pickle\n",
    "df = np.read_csv('mbti_1.csv')\n",
    "\n",
    "# populating types array\n",
    "labels = []\n",
    "for _type in df['type']:\n",
    "     labels.append(_type)\n",
    "\n",
    "with open('processed_posts.pkl', 'rb') as pf:\n",
    "    posts = pickle.load(pf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "countedPosts = [len(post) for post in posts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsByPost = []\n",
    "postsByPost = []\n",
    "\n",
    "for i in range(0, len(posts)):\n",
    "    for post in posts[i]:\n",
    "        labelsByPost.append(labels[i])\n",
    "        postsByPost.append(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing data\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "max_features = 10000\n",
    "max_len = 250\n",
    "batch_size = 32\n",
    "max_words = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 458 unique tokens\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=max_words,  char_level=True, lower=True)\n",
    "tokenizer.fit_on_texts(postsByPost)\n",
    "sequences = tokenizer.texts_to_sequences(postsByPost)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sequence.pad_sequences(sequences, maxlen=max_len)\n",
    "label_dictionary = {\n",
    "    'I': 0,\n",
    "    'E': 1\n",
    "}\n",
    "\n",
    "labels = [label_dictionary[label[0]] for label in labelsByPost]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "labels = np.asarray(labels).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (383869, 250)\n",
      "Shape of label tensor: (383869,)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Can't do this because it will lose track of the users posts\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the test labels and test data\n",
    "# from the rest, so we can keep track of users accordingly\n",
    "# Getting the first 800 users for testing\n",
    "# Getting users 800-1000 for validating\n",
    "testUsers = 35431\n",
    "valUsers = 8901\n",
    "\n",
    "x_test = data[:35431]\n",
    "y_test = labels[:35431]\n",
    "\n",
    "x_val = data[35431:35431+8901]\n",
    "y_val = labels[35431:35431+8901]\n",
    "\n",
    "data = data[35431+8901:]\n",
    "labels = labels[35431+8901:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_boundaries = countedPosts[:800]\n",
    "val_boundaries = countedPosts[800:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_labels = []\n",
    "e_data = []\n",
    "i_labels = []\n",
    "i_data = []\n",
    "\n",
    "for i in range(0, len(labels)):\n",
    "    if (labels[i] == 0):\n",
    "        i_labels.append(labels[i])\n",
    "        i_data.append(data[i])\n",
    "    if (labels[i] == 1):\n",
    "        e_labels.append(labels[i])\n",
    "        e_data.append(data[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mix_labels = e_labels + i_labels[:len(e_labels)]\n",
    "mix_data = e_data + i_data[:len(e_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mix_labels = np.asarray(mix_labels).astype('float32')\n",
    "mix_data = np.array(mix_data).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(mix_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "mix_data = mix_data[indices]\n",
    "mix_labels = mix_labels[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train = mix_data\n",
    "y_train = mix_labels"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('glove.6B.50d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "embedding_dim = 50\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "\n",
    "class True_eval(Callback):\n",
    "    def __init__(self, validation_data):\n",
    "        self.validation_data = validation_data\n",
    "        self.i_accuracy = []\n",
    "        self.e_accuracy = []\n",
    "        self.user_accuracy = []\n",
    "    \n",
    "    def ie_real_acc(self, prediction):\n",
    "        y_true = self.validation_data[1]\n",
    "        i_acc = 0\n",
    "        i_total = 0\n",
    "        e_acc = 0\n",
    "        e_total = 0\n",
    "        for i in range(0, len(prediction)):\n",
    "            if (y_true[i] == 0):\n",
    "                if (prediction[i].round() == y_true[i]):\n",
    "                    i_acc += 1\n",
    "                i_total += 1\n",
    "            else:\n",
    "                if (prediction[i].round() == y_true[i]):\n",
    "                    e_acc += 1\n",
    "                e_total += 1\n",
    "        return (i_acc/i_total), (e_acc/e_total)\n",
    "    \n",
    "    def user_acc(self, prediction):\n",
    "        y_true = self.validation_data[1]\n",
    "        usersCorrect = []\n",
    "        start = 0\n",
    "        for length in val_boundaries:\n",
    "            correct = 0\n",
    "            for i in range(0, length):\n",
    "                if (y_true[i+start] == prediction[i+start].round()):\n",
    "                    correct += 1\n",
    "            \n",
    "            usersCorrect.append(round(correct/length))\n",
    "            start += length\n",
    "            \n",
    "        print (usersCorrect)\n",
    "        return sum(usersCorrect)/len(usersCorrect)\n",
    "                \n",
    "        \n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x_val = self.validation_data[0]\n",
    "        y_pred = self.model.predict(x_val)\n",
    "        i_real_acc, e_real_acc = self.ie_real_acc(y_pred)\n",
    "        user_accuracy = self.user_acc(y_pred)\n",
    "        print (\"I Acc: %f\" % i_real_acc)\n",
    "        print (\"E Acc: %f\" % e_real_acc)\n",
    "        print (\"User Acc: %f\" % user_accuracy)\n",
    "        self.i_accuracy.append(i_real_acc)\n",
    "        self.e_accuracy.append(e_real_acc)\n",
    "        self.user_accuracy.append(user_accuracy)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('glove.6B.50d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "embedding_dim = 50\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.regularizers import l1_l2\n",
    "from keras.layers import Embedding, Dropout, LSTM, Dense\n",
    "\n",
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(Embedding(max_features, 128, input_length=max_len))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    #model.layers[0].set_weights([embedding_matrix])\n",
    "    #model.layers[0].trainable = False\n",
    "\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 160000 samples, validate on 8901 samples\n",
      "Epoch 1/100\n",
      "160000/160000 [==============================] - 864s 5ms/step - loss: 0.6933 - acc: 0.5040 - val_loss: 0.6944 - val_acc: 0.4847\n",
      "[0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0]\n",
      "I Acc: 0.478125\n",
      "E Acc: 0.509793\n",
      "User Acc: 0.400000\n",
      "Epoch 2/100\n",
      "160000/160000 [==============================] - 862s 5ms/step - loss: 0.6941 - acc: 0.5036 - val_loss: 0.6844 - val_acc: 0.6933\n",
      "[1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1]\n",
      "I Acc: 0.827977\n",
      "E Acc: 0.175734\n",
      "User Acc: 0.790000\n",
      "Epoch 3/100\n",
      "160000/160000 [==============================] - 884s 6ms/step - loss: 0.6937 - acc: 0.5054 - val_loss: 0.6830 - val_acc: 0.6978\n",
      "[1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1]\n",
      "I Acc: 0.833215\n",
      "E Acc: 0.177367\n",
      "User Acc: 0.795000\n",
      "Epoch 4/100\n",
      "160000/160000 [==============================] - 870s 5ms/step - loss: 0.6936 - acc: 0.5049 - val_loss: 0.6827 - val_acc: 0.7017\n",
      "[1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1]\n",
      "I Acc: 0.843976\n",
      "E Acc: 0.155060\n",
      "User Acc: 0.795000\n",
      "Epoch 5/100\n",
      "160000/160000 [==============================] - 863s 5ms/step - loss: 0.6934 - acc: 0.5068 - val_loss: 0.7007 - val_acc: 0.4277\n",
      "[0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0]\n",
      "I Acc: 0.376469\n",
      "E Acc: 0.624592\n",
      "User Acc: 0.240000\n",
      "Epoch 6/100\n",
      "160000/160000 [==============================] - 941s 6ms/step - loss: 0.6932 - acc: 0.5101 - val_loss: 0.6848 - val_acc: 0.6435\n",
      "[1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1]\n",
      "I Acc: 0.728444\n",
      "E Acc: 0.317193\n",
      "User Acc: 0.785000\n",
      "Epoch 7/100\n",
      "129792/160000 [=======================>......] - ETA: 2:38 - loss: 0.6934 - acc: 0.5093"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "model = build_model()\n",
    "with tf.device('/gpu:0'):\n",
    "    true_metrics = True_eval((x_val, y_val))\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        validation_data=(x_val, y_val),\n",
    "                        epochs=100, batch_size=256,\n",
    "                        callbacks=[true_metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_dict = history.history\n",
    "train_loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "train_acc = history_dict['categorical_accuracy']\n",
    "val_acc   = history_dict['val_categorical_accuracy']\n",
    "\n",
    "epochs = range(1, len(history_dict['categorical_accuracy']) + 1)\n",
    "\n",
    "plt.plot(epochs, train_loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "plt.plot(epochs, train_acc, 'bo', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation Accuracy')\n",
    "plt.title('Training and validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(epochs, true_metrics.i_accuracy, 'b', label='I Accuracy')\n",
    "plt.plot(epochs, true_metrics.e_accuracy, 'y', label='E Accuracy')\n",
    "plt.title('Letter Accuracies')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "URE18",
   "language": "python",
   "name": "ure18"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

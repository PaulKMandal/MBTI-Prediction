{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as np\n",
    "df = np.read_csv('mbti_1.csv')\n",
    "\n",
    "labels = []\n",
    "posts = []\n",
    "\n",
    "for _type in df['type']:\n",
    "    labels.append(_type)\n",
    "    \n",
    "for _posts in df['posts']:\n",
    "    posts.append(_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the posts\n",
    "posts = [post.split(\"|||\") for post in posts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting rid of url posts\n",
    "import re\n",
    "\n",
    "for index in range(0, len(posts)):\n",
    "    posts[index] = [post for post in posts[index] if not re.search(r'^(.)*http(.)*$', post)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting rid of number posts\n",
    "def isInt(s):\n",
    "    try: \n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "for index in range(0, len(posts)):\n",
    "    posts[index] = [post for post in posts[index] if not isInt(post)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing MBTI labels\n",
    "MBTI_regex = r'[\\w]*(i|e)(s|n)(f|t)(p|j)[\\w]*'\n",
    "for index in range(0, len(posts)):\n",
    "    posts[index] = [re.sub(MBTI_regex, '', post, flags=re.IGNORECASE)\n",
    "                    for post in posts[index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing empty Posts\n",
    "for index in range(0, len(posts)):\n",
    "    posts[index] = [post for post in posts[index] if post is not '' and post is not ' ' and len(post) > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENTATION Removing punctuation and stopwords\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "set_sw = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(post):\n",
    "    post = post.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(post)\n",
    "    filtered_words = filter(lambda token: token not in set_sw, tokens)\n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually Removing punctuation and stopwords\n",
    "for index in range(0, len(posts)):\n",
    "    posts[index] = [preprocess(post) for post in posts[index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing numbers from individual posts\n",
    "\n",
    "for index in range(0, len(posts)):\n",
    "    posts[index] = [re.sub(r'\\d+\\s', '', post) for post in posts[index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENTATION removing single letter words\n",
    "def removeSingleLetter(post):\n",
    "    tokens = post.split(\" \")\n",
    "    filtered_words = filter(lambda token: len(token) > 1, tokens)\n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually Removing single letter words\n",
    "\n",
    "for index in range(0, len(posts)):\n",
    "    posts[index] = [removeSingleLetter(post) for post in posts[index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENTATION Stemming the words in the post\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def stemPost(post):\n",
    "    tokens = post.split(\" \")\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually stemming the words in the posts\n",
    "for index in range(0, len(posts)):\n",
    "    posts[index] = [stemPost(post) for post in posts[index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing empty Posts one more time\n",
    "\n",
    "for index in range(0, len(posts)):\n",
    "    posts[index] = [post for post in posts[index] if post is not '' and post is not ' ' and len(post) > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the Data\n",
    "import pickle\n",
    "\n",
    "with open('processed_posts.pkl', 'wb') as file:\n",
    "    pickle.dump(posts, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "URE18",
   "language": "python",
   "name": "ure18"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

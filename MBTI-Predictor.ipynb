{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFile = open('formatted_data.json', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in dataFile:\n",
    "    data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = []\n",
    "posts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, len(data)):\n",
    "    types.append(data[i]['Type'])\n",
    "    posts.append(data[i]['Post'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# SEED = 673\n",
    "\n",
    "# random.seed(SEED)\n",
    "# random.shuffle(types)\n",
    "# random.shuffle(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_post_data(posts):\n",
    "    for index in range(0, len(posts)):\n",
    "        posts[index] = posts[index].split(\"|||\")\n",
    "        \n",
    "    return posts\n",
    "        \n",
    "posts = vectorize_post_data(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hyperTextLinksFromPosts(posts):\n",
    "    for index in range(0, len(posts)):\n",
    "        usable_post = [post for post in posts[index] if not re.search(r'^(.)*http(.)*$', post)]\n",
    "        posts[index] = usable_post\n",
    "    \n",
    "    return posts\n",
    "        \n",
    "posts = remove_hyperTextLinksFromPosts(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_MBTIClassifiersFromPosts(posts):\n",
    "    MBTI_regex = r'[\\w]*(i|e)(s|n)(f|t)(p|j)[\\w]*'\n",
    "    for i in range(0, len(posts)):\n",
    "        for j in range(0, len(posts[i])):\n",
    "            posts[i][j] = re.sub(MBTI_regex, ' ', posts[i][j], flags=re.IGNORECASE)\n",
    "        \n",
    "    return posts\n",
    "    \n",
    "posts = remove_MBTIClassifiersFromPosts(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def tokenize_posts(posts):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    for i in range(0, len(posts)):\n",
    "        user_words = []\n",
    "        for j in range(0, len(posts[i])):\n",
    "            post = tokenizer.tokenize(posts[i][j])\n",
    "            for word in post:\n",
    "                user_words.append(word)\n",
    "        posts[i] = user_words\n",
    "    \n",
    "    return posts\n",
    "\n",
    "posts = tokenize_posts(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_posts5 = []\n",
    "\n",
    "def isInt(s):\n",
    "    try: \n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def remove_numbers_in_posts(posts):\n",
    "    counter = 0\n",
    "    for index in range(0, len(posts)):\n",
    "        words = [word.lower() for word in posts[index] if not isInt(word)]\n",
    "        vectorized_posts5.append(words)\n",
    "        counter += 1\n",
    "        \n",
    "\n",
    "remove_numbers_in_posts(vectorized_posts4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_list = []\n",
    "\n",
    "for user in vectorized_posts5:\n",
    "    for word in user:\n",
    "        all_words_list.append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_list = Counter(all_words_list)\n",
    "dictionary = freq_list.most_common(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = list(zip(*dictionary))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = range(0, 10000)\n",
    "word_int = dict(zip(dictionary, nums))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vals = []\n",
    "\n",
    "for user in vectorized_posts5:\n",
    "    x_vals.append([word_int[x] for x in user if x in word_int.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_extro = [word[1:2] for word in types[1:len(types)]]\n",
    "bin_intro_extro = []\n",
    "for letter in intro_extro:\n",
    "    if (letter == 'S'):\n",
    "        bin_intro_extro.append(0)\n",
    "    else:\n",
    "        bin_intro_extro.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.array(x_vals)\n",
    "test_data = x[:1500]\n",
    "train_data = x[1500:]\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71750000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "y = np.asarray(bin_intro_extro).astype('float32')\n",
    "y_test = y[:1500]\n",
    "y_train = y[1500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7175"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:1500]\n",
    "x_partial_train = x_train[1500:]\n",
    "\n",
    "y_val = y_train[:1500]\n",
    "y_partial_train = y_train[1500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5675"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_partial_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape = (10000,)))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5675 samples, validate on 1500 samples\n",
      "Epoch 1/20\n",
      "5675/5675 [==============================] - 1s 170us/step - loss: 0.4414 - acc: 0.8169 - val_loss: 0.4616 - val_acc: 0.8520\n",
      "Epoch 2/20\n",
      "5675/5675 [==============================] - 1s 107us/step - loss: 0.3798 - acc: 0.8624 - val_loss: 0.5084 - val_acc: 0.8520\n",
      "Epoch 3/20\n",
      "5675/5675 [==============================] - 1s 109us/step - loss: 0.3546 - acc: 0.8624 - val_loss: 0.4363 - val_acc: 0.8520\n",
      "Epoch 4/20\n",
      "5675/5675 [==============================] - 1s 104us/step - loss: 0.3310 - acc: 0.8624 - val_loss: 0.4338 - val_acc: 0.8520\n",
      "Epoch 5/20\n",
      "5675/5675 [==============================] - 1s 110us/step - loss: 0.2860 - acc: 0.8626 - val_loss: 0.4793 - val_acc: 0.8520\n",
      "Epoch 6/20\n",
      "5675/5675 [==============================] - 1s 111us/step - loss: 0.2621 - acc: 0.8689 - val_loss: 0.4489 - val_acc: 0.8513\n",
      "Epoch 7/20\n",
      "5675/5675 [==============================] - 1s 104us/step - loss: 0.2428 - acc: 0.8925 - val_loss: 0.5269 - val_acc: 0.8513\n",
      "Epoch 8/20\n",
      "5675/5675 [==============================] - 1s 111us/step - loss: 0.2233 - acc: 0.9020 - val_loss: 0.5072 - val_acc: 0.8513\n",
      "Epoch 9/20\n",
      "5675/5675 [==============================] - 1s 122us/step - loss: 0.1713 - acc: 0.9241 - val_loss: 0.5907 - val_acc: 0.8513\n",
      "Epoch 10/20\n",
      "5675/5675 [==============================] - 1s 124us/step - loss: 0.1768 - acc: 0.9311 - val_loss: 0.6910 - val_acc: 0.8513\n",
      "Epoch 11/20\n",
      "5675/5675 [==============================] - 1s 113us/step - loss: 0.1323 - acc: 0.9503 - val_loss: 0.6592 - val_acc: 0.8513\n",
      "Epoch 12/20\n",
      "5675/5675 [==============================] - 1s 108us/step - loss: 0.1232 - acc: 0.9542 - val_loss: 0.7572 - val_acc: 0.8507\n",
      "Epoch 13/20\n",
      "5675/5675 [==============================] - 1s 108us/step - loss: 0.0776 - acc: 0.9824 - val_loss: 0.7004 - val_acc: 0.8447\n",
      "Epoch 14/20\n",
      "5675/5675 [==============================] - 1s 109us/step - loss: 0.1199 - acc: 0.9653 - val_loss: 0.7372 - val_acc: 0.8460\n",
      "Epoch 15/20\n",
      "5675/5675 [==============================] - 1s 111us/step - loss: 0.0423 - acc: 0.9974 - val_loss: 0.7710 - val_acc: 0.8440\n",
      "Epoch 16/20\n",
      "5675/5675 [==============================] - 1s 110us/step - loss: 0.1081 - acc: 0.9649 - val_loss: 0.7982 - val_acc: 0.8427\n",
      "Epoch 17/20\n",
      "5675/5675 [==============================] - 1s 110us/step - loss: 0.0265 - acc: 0.9996 - val_loss: 0.7724 - val_acc: 0.8307\n",
      "Epoch 18/20\n",
      "5675/5675 [==============================] - 1s 111us/step - loss: 0.0784 - acc: 0.9669 - val_loss: 0.9261 - val_acc: 0.8493\n",
      "Epoch 19/20\n",
      "5675/5675 [==============================] - 1s 108us/step - loss: 0.0270 - acc: 0.9959 - val_loss: 0.8697 - val_acc: 0.8360\n",
      "Epoch 20/20\n",
      "5675/5675 [==============================] - 1s 109us/step - loss: 0.0147 - acc: 0.9998 - val_loss: 0.8819 - val_acc: 0.8353\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_partial_train, y_partial_train, epochs = 20, batch_size = 512, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 103us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7880286078453064, 0.8459999995231628]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.evaluate(x_test, y_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape = (10000,)))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/13\n",
      "7175/7175 [==============================] - 1s 113us/step - loss: 0.4447 - acc: 0.8513\n",
      "Epoch 2/13\n",
      "7175/7175 [==============================] - 1s 85us/step - loss: 0.3868 - acc: 0.8602\n",
      "Epoch 3/13\n",
      "7175/7175 [==============================] - 1s 85us/step - loss: 0.3666 - acc: 0.8602\n",
      "Epoch 4/13\n",
      "7175/7175 [==============================] - 1s 85us/step - loss: 0.3202 - acc: 0.8602\n",
      "Epoch 5/13\n",
      "7175/7175 [==============================] - 1s 85us/step - loss: 0.3104 - acc: 0.8652\n",
      "Epoch 6/13\n",
      "7175/7175 [==============================] - 1s 82us/step - loss: 0.2849 - acc: 0.8711\n",
      "Epoch 7/13\n",
      "7175/7175 [==============================] - 1s 88us/step - loss: 0.2538 - acc: 0.8966\n",
      "Epoch 8/13\n",
      "7175/7175 [==============================] - 1s 105us/step - loss: 0.2228 - acc: 0.9111\n",
      "Epoch 9/13\n",
      "7175/7175 [==============================] - 1s 83us/step - loss: 0.1994 - acc: 0.9239\n",
      "Epoch 10/13\n",
      "7175/7175 [==============================] - 1s 83us/step - loss: 0.1762 - acc: 0.9338\n",
      "Epoch 11/13\n",
      "7175/7175 [==============================] - 1s 85us/step - loss: 0.1566 - acc: 0.9477\n",
      "Epoch 12/13\n",
      "7175/7175 [==============================] - 1s 130us/step - loss: 0.1420 - acc: 0.9560\n",
      "Epoch 13/13\n",
      "7175/7175 [==============================] - 1s 112us/step - loss: 0.1300 - acc: 0.9565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19c96a6d8>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs = 13, batch_size = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 162us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6722889490922292, 0.8646666661898295]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.evaluate(x_test, y_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "URE18",
   "language": "python",
   "name": "ure18"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

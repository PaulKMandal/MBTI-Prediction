{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file contains 8600ish users Data\n",
    "# In the form: {'MBTI Type', 'Social Media Posts'}\n",
    "dataFile = open('formatted_data.json', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the json data into a list\n",
    "for line in dataFile:\n",
    "    data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = []\n",
    "posts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating data into 2 separate lists for preprocessing\n",
    "# For the most part we process the 'post' data\n",
    "# we skip the first element as that is only the label\n",
    "for i in range(1, len(data)):\n",
    "    types.append(data[i]['Type'])\n",
    "    posts.append(data[i]['Post'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List shuffling to make sure our models are valid\n",
    "# Commented out for debugging purposes\n",
    "import random\n",
    "SEED = 673\n",
    "\n",
    "random.seed(SEED)\n",
    "random.shuffle(types)\n",
    "random.shuffle(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning the posts from: 'post1|||post2|||post3'\n",
    "#                     to: ['post1', 'post2', 'post3']\n",
    "# expects a list of posts as strings\n",
    "# returns a list of lists of posts\n",
    "def vectorize_post_data(posts):\n",
    "    for index in range(0, len(posts)):\n",
    "        posts[index] = posts[index].split(\"|||\")\n",
    "        \n",
    "    return posts\n",
    "        \n",
    "posts = vectorize_post_data(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hyperTextLinksFromPosts(posts):\n",
    "    for index in range(0, len(posts)):\n",
    "        usable_post = [post for post in posts[index] if not re.search(r'^(.)*http(.)*$', post)]\n",
    "        posts[index] = usable_post\n",
    "    \n",
    "    return posts\n",
    "        \n",
    "posts = remove_hyperTextLinksFromPosts(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_MBTIClassifiersFromPosts(posts):\n",
    "    MBTI_regex = r'[\\w]*(i|e)(s|n)(f|t)(p|j)[\\w]*'\n",
    "    for i in range(0, len(posts)):\n",
    "        for j in range(0, len(posts[i])):\n",
    "            posts[i][j] = re.sub(MBTI_regex, ' ', posts[i][j], flags=re.IGNORECASE)\n",
    "        \n",
    "    return posts\n",
    "    \n",
    "posts = remove_MBTIClassifiersFromPosts(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def isInt(s):\n",
    "    try: \n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def tokenize_posts(posts):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    for i in range(0, len(posts)):\n",
    "        user_words = []\n",
    "        for j in range(0, len(posts[i])):\n",
    "            post = tokenizer.tokenize(posts[i][j])\n",
    "            for word in post:\n",
    "                if not isInt(word) and len(word) > 1:\n",
    "                    user_words.append(word.lower())\n",
    "                user_words = [w for w in user_words if not w in stop_words]\n",
    "        posts[i] = user_words\n",
    "    \n",
    "    return posts\n",
    "\n",
    "posts = tokenize_posts(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_list = []\n",
    "\n",
    "for user in posts:\n",
    "    for word in user:\n",
    "        all_words_list.append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_list = Counter(all_words_list)\n",
    "dictionary = freq_list.most_common(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = list(zip(*dictionary))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = range(0, 10000)\n",
    "word_int = dict(zip(dictionary, nums))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vals = []\n",
    "\n",
    "for user in posts:\n",
    "    x_vals.append([word_int[x] for x in user if x in word_int.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_extro = [word[0:1] for word in types[0:len(types)]]\n",
    "bin_intro_extro = []\n",
    "for letter in intro_extro:\n",
    "    if (letter == 'I'):\n",
    "        bin_intro_extro.append(0)\n",
    "    else:\n",
    "        bin_intro_extro.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.array(x_vals)\n",
    "test_data = x[:1500]\n",
    "train_data = x[1500:]\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "y = np.asarray(bin_intro_extro).astype('float32')\n",
    "y_test = y[:1500]\n",
    "y_train = y[1500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:1500]\n",
    "x_partial_train = x_train[1500:]\n",
    "\n",
    "y_val = y_train[:1500]\n",
    "y_partial_train = y_train[1500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "\n",
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(16, activation='relu', input_shape = (10000,)))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(16, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing fold # 0\n",
      "1793/1793 [==============================] - 2s 1ms/step\n",
      "Accuracy: 0.781372002230898\n",
      "processing fold # 1\n",
      "1793/1793 [==============================] - 2s 1ms/step\n",
      "Accuracy: 0.7791411042944786\n",
      "processing fold # 2\n",
      "1793/1793 [==============================] - 2s 1ms/step\n",
      "Accuracy: 0.7657557166759621\n",
      "processing fold # 3\n",
      "1793/1793 [==============================] - 2s 1ms/step\n",
      "Accuracy: 0.7512548800892359\n",
      "Accuracy Mean: 0.7693809258226437\n"
     ]
    }
   ],
   "source": [
    "#Iterated K-Fold Validation\n",
    "k = 4\n",
    "\n",
    "# // is floor div operator\n",
    "num_val_samples = len(x_train) // k\n",
    "num_epochs = 20\n",
    "all_scores = []\n",
    "\n",
    "for i in range(1):\n",
    "    random.shuffle(x_train)\n",
    "    random.shuffle(y_train)\n",
    "    for i in range(k):\n",
    "        print('processing fold #', i)\n",
    "        val_data = x_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "        val_targets = y_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "        partial_train_data = np.concatenate(\n",
    "            [x_train[:i * num_val_samples],\n",
    "             x_train[(i + 1) * num_val_samples:]],\n",
    "            axis=0)\n",
    "        partial_train_targets = np.concatenate(\n",
    "            [y_train[:i * num_val_samples],\n",
    "             y_train[(i + 1) * num_val_samples:]],\n",
    "            axis=0)\n",
    "\n",
    "        model = build_model()\n",
    "        with tf.device('/gpu:0'):\n",
    "            model.fit(partial_train_data, partial_train_targets,\n",
    "                epochs=num_epochs, batch_size=len(partial_train_data), verbose=0)\n",
    "            \n",
    "            val_loss, val_acc = model.evaluate(val_data, val_targets)\n",
    "        print(\"Accuracy:\", val_acc)\n",
    "        all_scores.append(val_acc)\n",
    "    \n",
    "print (\"Accuracy Mean:\", np.mean(all_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "7175/7175 [==============================] - 4s 608us/step - loss: 0.5797 - acc: 0.7703\n",
      "Epoch 2/25\n",
      "7175/7175 [==============================] - 1s 100us/step - loss: 0.5516 - acc: 0.7711\n",
      "Epoch 3/25\n",
      "7175/7175 [==============================] - 1s 104us/step - loss: 0.5429 - acc: 0.7711\n",
      "Epoch 4/25\n",
      "7175/7175 [==============================] - 1s 98us/step - loss: 0.5425 - acc: 0.7714\n",
      "Epoch 5/25\n",
      "7175/7175 [==============================] - 1s 107us/step - loss: 0.5519 - acc: 0.7714\n",
      "Epoch 6/25\n",
      "7175/7175 [==============================] - 1s 101us/step - loss: 0.5431 - acc: 0.7716\n",
      "Epoch 7/25\n",
      "7175/7175 [==============================] - 1s 104us/step - loss: 0.5416 - acc: 0.7717\n",
      "Epoch 8/25\n",
      "7175/7175 [==============================] - 1s 104us/step - loss: 0.5410 - acc: 0.7716\n",
      "Epoch 9/25\n",
      "7175/7175 [==============================] - 1s 99us/step - loss: 0.5398 - acc: 0.7717\n",
      "Epoch 10/25\n",
      "7175/7175 [==============================] - 1s 97us/step - loss: 0.5441 - acc: 0.7718\n",
      "Epoch 11/25\n",
      "7175/7175 [==============================] - 1s 91us/step - loss: 0.5399 - acc: 0.7717\n",
      "Epoch 12/25\n",
      "7175/7175 [==============================] - 1s 97us/step - loss: 0.5419 - acc: 0.7717\n",
      "Epoch 13/25\n",
      "7175/7175 [==============================] - 1s 100us/step - loss: 0.5423 - acc: 0.7717\n",
      "Epoch 14/25\n",
      "7175/7175 [==============================] - 1s 98us/step - loss: 0.5388 - acc: 0.7717\n",
      "Epoch 15/25\n",
      "7175/7175 [==============================] - 1s 96us/step - loss: 0.5433 - acc: 0.7711\n",
      "Epoch 16/25\n",
      "7175/7175 [==============================] - 1s 99us/step - loss: 0.5394 - acc: 0.7714\n",
      "Epoch 17/25\n",
      "7175/7175 [==============================] - 1s 97us/step - loss: 0.5407 - acc: 0.7717\n",
      "Epoch 18/25\n",
      "7175/7175 [==============================] - 1s 97us/step - loss: 0.5390 - acc: 0.7714\n",
      "Epoch 19/25\n",
      "7175/7175 [==============================] - 1s 101us/step - loss: 0.5399 - acc: 0.7718\n",
      "Epoch 20/25\n",
      "7175/7175 [==============================] - 1s 92us/step - loss: 0.5381 - acc: 0.7716\n",
      "Epoch 21/25\n",
      "7175/7175 [==============================] - 1s 87us/step - loss: 0.5370 - acc: 0.7717\n",
      "Epoch 22/25\n",
      "7175/7175 [==============================] - 1s 100us/step - loss: 0.5366 - acc: 0.7717\n",
      "Epoch 23/25\n",
      "7175/7175 [==============================] - 1s 102us/step - loss: 0.5384 - acc: 0.7717\n",
      "Epoch 24/25\n",
      "7175/7175 [==============================] - 1s 96us/step - loss: 0.5382 - acc: 0.7717\n",
      "Epoch 25/25\n",
      "7175/7175 [==============================] - 1s 92us/step - loss: 0.5361 - acc: 0.7717\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "with tf.device('/gpu:0'):\n",
    "    model.fit(x_train, y_train, epochs = 25, batch_size = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 2s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5835738066037496, 0.7620000003178914]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.evaluate(x_test, y_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "URE18_HOME",
   "language": "python",
   "name": "ure18_home"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

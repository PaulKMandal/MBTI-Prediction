{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFile = open('formatted_data.json', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in dataFile:\n",
    "    data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = []\n",
    "posts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, len(data)):\n",
    "    types.append(data[i]['Type'])\n",
    "    posts.append(data[i]['Post'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# SEED = 673\n",
    "\n",
    "# random.seed(SEED)\n",
    "# random.shuffle(types)\n",
    "# random.shuffle(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_post_data(posts):\n",
    "    for index in range(0, len(posts)):\n",
    "        posts[index] = posts[index].split(\"|||\")\n",
    "        \n",
    "    return posts\n",
    "        \n",
    "posts = vectorize_post_data(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hyperTextLinksFromPosts(posts):\n",
    "    for index in range(0, len(posts)):\n",
    "        usable_post = [post for post in posts[index] if not re.search(r'^(.)*http(.)*$', post)]\n",
    "        posts[index] = usable_post\n",
    "    \n",
    "    return posts\n",
    "        \n",
    "posts = remove_hyperTextLinksFromPosts(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 . What has been the most life-changing experience in your life?\n",
      "1 . May the PerC Experience immerse you.\n",
      "2 . Hello ENFJ7. Sorry to hear of your distress. It's only natural for a relationship to not be perfection all the time in every moment of existence. Try to figure the hard times as times of growth, as...\n",
      "3 . Welcome and stuff.\n",
      "4 . Prozac, wellbrutin, at least thirty minutes of moving your legs (and I don't mean moving them while sitting in your same desk chair), weed in moderation (maybe try edibles as a healthier alternative...\n",
      "5 . Basically come up with three items you've determined that each type (or whichever types you want to do) would more than likely use, given each types' cognitive functions and whatnot, when left by...\n",
      "6 . All things in moderation.  Sims is indeed a video game, and a good one at that. Note: a good one at that is somewhat subjective in that I am not completely promoting the death of any given Sim...\n",
      "7 . Dear ENFP:  What were your favorite video games growing up and what are your now, current favorite video games? :cool:\n",
      "8 . It appears to be too late. :sad:\n",
      "9 . There's someone out there for everyone.\n",
      "10 . Wait... I thought confidence was a good thing.\n",
      "11 . I just cherish the time of solitude b/c i revel within my inner world more whereas most other time i'd be workin... just enjoy the me time while you can. Don't worry, people will always be around to...\n",
      "12 . Yo entp ladies... if you're into a complimentary personality,well, hey.\n",
      "13 . ... when your main social outlet is xbox live conversations and even then you verbally fatigue quickly.\n",
      "14 . Banned because this thread requires it of me.\n",
      "15 . Get high in backyard, roast and eat marshmellows in backyard while conversing over something intellectual, followed by massages and kisses.\n",
      "16 . Banned for too many b's in that sentence. How could you! Think of the B!\n",
      "17 . Banned for watching movies in the corner with the dunces.\n",
      "18 . Banned because Health class clearly taught you nothing about peer pressure.\n",
      "19 . Banned for a whole host of reasons!\n",
      "20 . 1) Two baby deer on left and right munching on a beetle in the middle.  2) Using their own blood, two cavemen diary today's latest happenings on their designated cave diary wall.  3) I see it as...\n",
      "21 . a pokemon world  an infj society  everyone becomes an optimist\n",
      "22 . 49142\n",
      "23 . Not all artists are artists because they draw. It's the idea that counts in forming something of your own... like a signature.\n",
      "24 . Welcome to the robot ranks, person who downed my self-esteem cuz I'm not an avid signature artist like herself. :proud:\n",
      "25 . Banned for taking all the room under my bed. Ya gotta learn to share with the roaches.\n",
      "26 . Banned for being too much of a thundering, grumbling kind of storm... yep.\n",
      "27 . I failed a public speaking class a few years ago and I've sort of learned what I could do better were I to be in that position again. A big part of my failure was just overloading myself with too...\n",
      "28 . Move to the Denver area and start a new life for myself.'\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for post in posts[0]:\n",
    "    print (counter, '.' ,post)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_MBTIClassifiersFromPosts(posts):\n",
    "    MBTI_regex = r'[\\w]*(i|e)(s|n)(f|t)(p|j)[\\w]*'\n",
    "    for i in range(0, len(posts)):\n",
    "        for j in range(0, len(posts[i])):\n",
    "            posts[i][j] = re.sub(MBTI_regex, ' ', posts[i][j], flags=re.IGNORECASE)\n",
    "        \n",
    "    return posts\n",
    "    \n",
    "posts = remove_MBTIClassifiersFromPosts(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 . What has been the most life-changing experience in your life?\n",
      "1 . May the PerC Experience immerse you.\n",
      "2 . Hello  . Sorry to hear of your distress. It's only natural for a relationship to not be perfection all the time in every moment of existence. Try to figure the hard times as times of growth, as...\n",
      "3 . Welcome and stuff.\n",
      "4 . Prozac, wellbrutin, at least thirty minutes of moving your legs (and I don't mean moving them while sitting in your same desk chair), weed in moderation (maybe try edibles as a healthier alternative...\n",
      "5 . Basically come up with three items you've determined that each type (or whichever types you want to do) would more than likely use, given each types' cognitive functions and whatnot, when left by...\n",
      "6 . All things in moderation.  Sims is indeed a video game, and a good one at that. Note: a good one at that is somewhat subjective in that I am not completely promoting the death of any given Sim...\n",
      "7 . Dear  :  What were your favorite video games growing up and what are your now, current favorite video games? :cool:\n",
      "8 . It appears to be too late. :sad:\n",
      "9 . There's someone out there for everyone.\n",
      "10 . Wait... I thought confidence was a good thing.\n",
      "11 . I just cherish the time of solitude b/c i revel within my inner world more whereas most other time i'd be workin... just enjoy the me time while you can. Don't worry, people will always be around to...\n",
      "12 . Yo   ladies... if you're into a complimentary personality,well, hey.\n",
      "13 . ... when your main social outlet is xbox live conversations and even then you verbally fatigue quickly.\n",
      "14 . Banned because this thread requires it of me.\n",
      "15 . Get high in backyard, roast and eat marshmellows in backyard while conversing over something intellectual, followed by massages and kisses.\n",
      "16 . Banned for too many b's in that sentence. How could you! Think of the B!\n",
      "17 . Banned for watching movies in the corner with the dunces.\n",
      "18 . Banned because Health class clearly taught you nothing about peer pressure.\n",
      "19 . Banned for a whole host of reasons!\n",
      "20 . 1) Two baby deer on left and right munching on a beetle in the middle.  2) Using their own blood, two cavemen diary today's latest happenings on their designated cave diary wall.  3) I see it as...\n",
      "21 . a pokemon world  an   society  everyone becomes an optimist\n",
      "22 . 49142\n",
      "23 . Not all artists are artists because they draw. It's the idea that counts in forming something of your own... like a signature.\n",
      "24 . Welcome to the robot ranks, person who downed my self-esteem cuz I'm not an avid signature artist like herself. :proud:\n",
      "25 . Banned for taking all the room under my bed. Ya gotta learn to share with the roaches.\n",
      "26 . Banned for being too much of a thundering, grumbling kind of storm... yep.\n",
      "27 . I failed a public speaking class a few years ago and I've sort of learned what I could do better were I to be in that position again. A big part of my failure was just overloading myself with too...\n",
      "28 . Move to the Denver area and start a new life for myself.'\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for post in posts[0]:\n",
    "    print (counter, '.' ,post)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def tokenize_posts(posts):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    for i in range(0, len(posts)):\n",
    "        user_words = []\n",
    "        for j in range(0, len(posts[i])):\n",
    "            post = tokenizer.tokenize(posts[i][j])\n",
    "            for word in post:\n",
    "                user_words.append(word)\n",
    "        posts[i] = user_words\n",
    "    \n",
    "    return posts\n",
    "\n",
    "posts = tokenize_posts(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_posts5 = []\n",
    "\n",
    "def isInt(s):\n",
    "    try: \n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def remove_numbers_in_posts(posts):\n",
    "    counter = 0\n",
    "    for index in range(0, len(posts)):\n",
    "        words = [word.lower() for word in posts[index] if not isInt(word)]\n",
    "        vectorized_posts5.append(words)\n",
    "        counter += 1\n",
    "        \n",
    "\n",
    "remove_numbers_in_posts(vectorized_posts4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_list = []\n",
    "\n",
    "for user in vectorized_posts5:\n",
    "    for word in user:\n",
    "        all_words_list.append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_list = Counter(all_words_list)\n",
    "dictionary = freq_list.most_common(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = list(zip(*dictionary))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = range(0, 10000)\n",
    "word_int = dict(zip(dictionary, nums))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vals = []\n",
    "\n",
    "for user in vectorized_posts5:\n",
    "    x_vals.append([word_int[x] for x in user if x in word_int.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_extro = [word[1:2] for word in types[1:len(types)]]\n",
    "bin_intro_extro = []\n",
    "for letter in intro_extro:\n",
    "    if (letter == 'S'):\n",
    "        bin_intro_extro.append(0)\n",
    "    else:\n",
    "        bin_intro_extro.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.array(x_vals)\n",
    "test_data = x[:1500]\n",
    "train_data = x[1500:]\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71750000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "y = np.asarray(bin_intro_extro).astype('float32')\n",
    "y_test = y[:1500]\n",
    "y_train = y[1500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7175"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:1500]\n",
    "x_partial_train = x_train[1500:]\n",
    "\n",
    "y_val = y_train[:1500]\n",
    "y_partial_train = y_train[1500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5675"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_partial_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape = (10000,)))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5675 samples, validate on 1500 samples\n",
      "Epoch 1/20\n",
      "5675/5675 [==============================] - 1s 170us/step - loss: 0.4414 - acc: 0.8169 - val_loss: 0.4616 - val_acc: 0.8520\n",
      "Epoch 2/20\n",
      "5675/5675 [==============================] - 1s 107us/step - loss: 0.3798 - acc: 0.8624 - val_loss: 0.5084 - val_acc: 0.8520\n",
      "Epoch 3/20\n",
      "5675/5675 [==============================] - 1s 109us/step - loss: 0.3546 - acc: 0.8624 - val_loss: 0.4363 - val_acc: 0.8520\n",
      "Epoch 4/20\n",
      "5675/5675 [==============================] - 1s 104us/step - loss: 0.3310 - acc: 0.8624 - val_loss: 0.4338 - val_acc: 0.8520\n",
      "Epoch 5/20\n",
      "5675/5675 [==============================] - 1s 110us/step - loss: 0.2860 - acc: 0.8626 - val_loss: 0.4793 - val_acc: 0.8520\n",
      "Epoch 6/20\n",
      "5675/5675 [==============================] - 1s 111us/step - loss: 0.2621 - acc: 0.8689 - val_loss: 0.4489 - val_acc: 0.8513\n",
      "Epoch 7/20\n",
      "5675/5675 [==============================] - 1s 104us/step - loss: 0.2428 - acc: 0.8925 - val_loss: 0.5269 - val_acc: 0.8513\n",
      "Epoch 8/20\n",
      "5675/5675 [==============================] - 1s 111us/step - loss: 0.2233 - acc: 0.9020 - val_loss: 0.5072 - val_acc: 0.8513\n",
      "Epoch 9/20\n",
      "5675/5675 [==============================] - 1s 122us/step - loss: 0.1713 - acc: 0.9241 - val_loss: 0.5907 - val_acc: 0.8513\n",
      "Epoch 10/20\n",
      "5675/5675 [==============================] - 1s 124us/step - loss: 0.1768 - acc: 0.9311 - val_loss: 0.6910 - val_acc: 0.8513\n",
      "Epoch 11/20\n",
      "5675/5675 [==============================] - 1s 113us/step - loss: 0.1323 - acc: 0.9503 - val_loss: 0.6592 - val_acc: 0.8513\n",
      "Epoch 12/20\n",
      "5675/5675 [==============================] - 1s 108us/step - loss: 0.1232 - acc: 0.9542 - val_loss: 0.7572 - val_acc: 0.8507\n",
      "Epoch 13/20\n",
      "5675/5675 [==============================] - 1s 108us/step - loss: 0.0776 - acc: 0.9824 - val_loss: 0.7004 - val_acc: 0.8447\n",
      "Epoch 14/20\n",
      "5675/5675 [==============================] - 1s 109us/step - loss: 0.1199 - acc: 0.9653 - val_loss: 0.7372 - val_acc: 0.8460\n",
      "Epoch 15/20\n",
      "5675/5675 [==============================] - 1s 111us/step - loss: 0.0423 - acc: 0.9974 - val_loss: 0.7710 - val_acc: 0.8440\n",
      "Epoch 16/20\n",
      "5675/5675 [==============================] - 1s 110us/step - loss: 0.1081 - acc: 0.9649 - val_loss: 0.7982 - val_acc: 0.8427\n",
      "Epoch 17/20\n",
      "5675/5675 [==============================] - 1s 110us/step - loss: 0.0265 - acc: 0.9996 - val_loss: 0.7724 - val_acc: 0.8307\n",
      "Epoch 18/20\n",
      "5675/5675 [==============================] - 1s 111us/step - loss: 0.0784 - acc: 0.9669 - val_loss: 0.9261 - val_acc: 0.8493\n",
      "Epoch 19/20\n",
      "5675/5675 [==============================] - 1s 108us/step - loss: 0.0270 - acc: 0.9959 - val_loss: 0.8697 - val_acc: 0.8360\n",
      "Epoch 20/20\n",
      "5675/5675 [==============================] - 1s 109us/step - loss: 0.0147 - acc: 0.9998 - val_loss: 0.8819 - val_acc: 0.8353\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_partial_train, y_partial_train, epochs = 20, batch_size = 512, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 103us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7880286078453064, 0.8459999995231628]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.evaluate(x_test, y_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation='relu', input_shape = (10000,)))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/13\n",
      "7175/7175 [==============================] - 1s 113us/step - loss: 0.4447 - acc: 0.8513\n",
      "Epoch 2/13\n",
      "7175/7175 [==============================] - 1s 85us/step - loss: 0.3868 - acc: 0.8602\n",
      "Epoch 3/13\n",
      "7175/7175 [==============================] - 1s 85us/step - loss: 0.3666 - acc: 0.8602\n",
      "Epoch 4/13\n",
      "7175/7175 [==============================] - 1s 85us/step - loss: 0.3202 - acc: 0.8602\n",
      "Epoch 5/13\n",
      "7175/7175 [==============================] - 1s 85us/step - loss: 0.3104 - acc: 0.8652\n",
      "Epoch 6/13\n",
      "7175/7175 [==============================] - 1s 82us/step - loss: 0.2849 - acc: 0.8711\n",
      "Epoch 7/13\n",
      "7175/7175 [==============================] - 1s 88us/step - loss: 0.2538 - acc: 0.8966\n",
      "Epoch 8/13\n",
      "7175/7175 [==============================] - 1s 105us/step - loss: 0.2228 - acc: 0.9111\n",
      "Epoch 9/13\n",
      "7175/7175 [==============================] - 1s 83us/step - loss: 0.1994 - acc: 0.9239\n",
      "Epoch 10/13\n",
      "7175/7175 [==============================] - 1s 83us/step - loss: 0.1762 - acc: 0.9338\n",
      "Epoch 11/13\n",
      "7175/7175 [==============================] - 1s 85us/step - loss: 0.1566 - acc: 0.9477\n",
      "Epoch 12/13\n",
      "7175/7175 [==============================] - 1s 130us/step - loss: 0.1420 - acc: 0.9560\n",
      "Epoch 13/13\n",
      "7175/7175 [==============================] - 1s 112us/step - loss: 0.1300 - acc: 0.9565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x19c96a6d8>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs = 13, batch_size = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 162us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6722889490922292, 0.8646666661898295]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = model.evaluate(x_test, y_test)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "URE18",
   "language": "python",
   "name": "ure18"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

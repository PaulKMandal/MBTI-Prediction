{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward Neural Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 600\n",
    "# 1st Layer\n",
    "LAYER1_SIZE = 16\n",
    "LAYER1_ACTIVATION = 'relu'\n",
    "LAYER1_INPUT_DIMENSION = 10000\n",
    "\n",
    "LAYER1_PARAMS = [str(LAYER1_SIZE), LAYER1_ACTIVATION, LAYER1_INPUT_DIMENSION]\n",
    "\n",
    "# 2nd LAYER\n",
    "LAYER2_SIZE = 16\n",
    "LAYER2_ACTIVATION = 'relu'\n",
    "\n",
    "LAYER2_PARAMS = [str(LAYER2_SIZE), LAYER2_ACTIVATION]\n",
    "\n",
    "# 3rd LAYER\n",
    "LAYER3_SIZE = 1\n",
    "LAYER3_ACTIVATION = 'sigmoid'\n",
    "\n",
    "LAYER3_PARAMS = [str(LAYER3_SIZE), LAYER3_ACTIVATION]\n",
    "\n",
    "# Geralizers\n",
    "DROPOUT_RATE = 0.5\n",
    "L1 = 0.001\n",
    "L2 = 0.001\n",
    "\n",
    "# TRAINING\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "TRAIN_PARAMS = [EPOCHS, BATCH_SIZE]\n",
    "\n",
    "# COMPILATION\n",
    "OPTIMIZER = 'rmsprop'\n",
    "LOSS = 'binary_crossentropy'\n",
    "METRICS = 'accuracy'\n",
    "\n",
    "COMPILATION_PARAMS = [OPTIMIZER, LOSS, METRICS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file contains 8600ish users Data\n",
    "# In the form: {'MBTI Type', 'Social Media Posts'}\n",
    "dataFile = open('formatted_data.json', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the json data into a list\n",
    "for line in dataFile:\n",
    "    data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = []\n",
    "posts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating data into 2 separate lists for preprocessing\n",
    "# For the most part we process the 'post' data\n",
    "# we skip the first element as that is only the label\n",
    "for i in range(1, len(data)):\n",
    "    types.append(data[i]['Type'])\n",
    "    posts.append(data[i]['Post'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning the posts from: 'post1|||post2|||post3'\n",
    "#                     to: ['post1', 'post2', 'post3']\n",
    "# expects a list of posts as strings\n",
    "# returns a list of lists of posts\n",
    "def vectorize_post_data(posts):\n",
    "    for index in range(0, len(posts)):\n",
    "        posts[index] = posts[index].split(\"|||\")\n",
    "        \n",
    "    return posts\n",
    "\n",
    "posts = vectorize_post_data(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hyperTextLinksFromPosts(posts):\n",
    "    for index in range(0, len(posts)):\n",
    "        usable_post = [post for post in posts[index] if not re.search(r'^(.)*http(.)*$', post)]\n",
    "        posts[index] = usable_post\n",
    "    \n",
    "    return posts\n",
    "        \n",
    "posts = remove_hyperTextLinksFromPosts(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_MBTIClassifiersFromPosts(posts):\n",
    "    MBTI_regex = r'[\\w]*(i|e)(s|n)(f|t)(p|j)[\\w]*'\n",
    "    for i in range(0, len(posts)):\n",
    "        for j in range(0, len(posts[i])):\n",
    "            posts[i][j] = re.sub(MBTI_regex, ' ', posts[i][j], flags=re.IGNORECASE)\n",
    "        \n",
    "    return posts\n",
    "    \n",
    "posts = remove_MBTIClassifiersFromPosts(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "maxlen = 50\n",
    "max_words = 10000\n",
    "tokenizer = Tokenizer(num_words = max_words)\n",
    "tokenizer.fit_on_texts(posts)\n",
    "sequences = tokenizer.texts_to_sequences(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 391958 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' %len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_data = pad_sequences(sequences, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of data tensor: (8675, 50)\n",
      "shape of label tensor (8675,)\n"
     ]
    }
   ],
   "source": [
    "labels = np.asarray(types)\n",
    "print('shape of data tensor:', posts_data.shape)\n",
    "print('shape of label tensor', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['INFJ', 'ENTP', 'INTP', ..., 'INTP', 'INFP', 'INFP'], dtype='<U4')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(SEED)\n",
    "random.shuffle(posts_data)\n",
    "random.seed(SEED)\n",
    "random.shuffle(labels)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def isInt(s):\n",
    "    try: \n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def tokenize_posts(posts):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    for i in range(0, len(posts)):\n",
    "        user_words = []\n",
    "        for j in range(0, len(posts[i])):\n",
    "            post = tokenizer.tokenize(posts[i][j])\n",
    "            for word in post:\n",
    "                if not isInt(word) and len(word) > 1:\n",
    "                    user_words.append(word.lower())\n",
    "                user_words = [w for w in user_words if not w in stop_words]\n",
    "        posts[i] = user_words\n",
    "    \n",
    "    return posts\n",
    "\n",
    "posts = tokenize_posts(posts)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import pickle\n",
    "\n",
    "tokenized_posts = []\n",
    "with open ('tokenized_formatted_data.txt', 'rb') as fp:\n",
    "    tokenized_posts = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "all_words_list = []\n",
    "\n",
    "for user in tokenized_posts:\n",
    "    for word in user:\n",
    "        all_words_list.append(word)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "freq_list = Counter(all_words_list)\n",
    "dictionary = freq_list.most_common(10000)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dictionary = list(zip(*dictionary))[0]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "nums = range(0, 10000)\n",
    "word_int = dict(zip(dictionary, nums))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x_vals = []\n",
    "\n",
    "for user in tokenized_posts:\n",
    "    x_vals.append([word_int[x] for x in user if x in word_int.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_extro = [word[0:1] for word in types[0:len(labels)]]\n",
    "bin_intro_extro = []\n",
    "for letter in intro_extro:\n",
    "    if (letter == 'I'):\n",
    "        bin_intro_extro.append(0)\n",
    "    else:\n",
    "        bin_intro_extro.append(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8675"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bin_intro_extro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = posts_data[1500:]\n",
    "y_train = bin_intro_extro[1500:]\n",
    "x_val = x_train[1500:]\n",
    "y_val = y_train[1500:]\n",
    "x_test = posts_data[:1500]\n",
    "y_test = bin_intro_extro[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "f = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype = 'float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('LSTM_formated_data.txt', 'wb') as fp:\n",
    "    pickle.dump(posts_data, fp)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8675"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(posts_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i<max_words:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7175/7175 [==============================] - 5s 754us/step - loss: 0.5547 - acc: 0.7659\n",
      "Epoch 2/20\n",
      "7175/7175 [==============================] - 5s 647us/step - loss: 0.5408 - acc: 0.7689\n",
      "Epoch 3/20\n",
      "7175/7175 [==============================] - 5s 665us/step - loss: 0.5382 - acc: 0.7689\n",
      "Epoch 4/20\n",
      "7175/7175 [==============================] - 5s 661us/step - loss: 0.5345 - acc: 0.7688\n",
      "Epoch 5/20\n",
      "7175/7175 [==============================] - 5s 661us/step - loss: 0.5300 - acc: 0.7699 1s - loss: 0\n",
      "Epoch 6/20\n",
      "7175/7175 [==============================] - 5s 658us/step - loss: 0.5250 - acc: 0.7716\n",
      "Epoch 7/20\n",
      "7175/7175 [==============================] - 5s 660us/step - loss: 0.5191 - acc: 0.7728\n",
      "Epoch 8/20\n",
      "7175/7175 [==============================] - 5s 656us/step - loss: 0.5129 - acc: 0.7749\n",
      "Epoch 9/20\n",
      "7175/7175 [==============================] - 5s 663us/step - loss: 0.5075 - acc: 0.7790\n",
      "Epoch 10/20\n",
      "7175/7175 [==============================] - 5s 671us/step - loss: 0.5034 - acc: 0.7833\n",
      "Epoch 11/20\n",
      "7175/7175 [==============================] - 5s 704us/step - loss: 0.5005 - acc: 0.7880\n",
      "Epoch 12/20\n",
      "7175/7175 [==============================] - 5s 743us/step - loss: 0.4981 - acc: 0.7894\n",
      "Epoch 13/20\n",
      "7175/7175 [==============================] - 5s 696us/step - loss: 0.4956 - acc: 0.7908\n",
      "Epoch 14/20\n",
      "7175/7175 [==============================] - 5s 676us/step - loss: 0.4940 - acc: 0.7908\n",
      "Epoch 15/20\n",
      "7175/7175 [==============================] - 5s 675us/step - loss: 0.4922 - acc: 0.7919\n",
      "Epoch 16/20\n",
      "7175/7175 [==============================] - 5s 685us/step - loss: 0.4914 - acc: 0.7930\n",
      "Epoch 17/20\n",
      "7175/7175 [==============================] - 5s 685us/step - loss: 0.4904 - acc: 0.7939\n",
      "Epoch 18/20\n",
      "7175/7175 [==============================] - 5s 683us/step - loss: 0.4900 - acc: 0.7919\n",
      "Epoch 19/20\n",
      "7175/7175 [==============================] - 5s 685us/step - loss: 0.4884 - acc: 0.7928 1s - loss: 0.\n",
      "Epoch 20/20\n",
      "7175/7175 [==============================] - 5s 688us/step - loss: 0.4882 - acc: 0.7934\n",
      "1500/1500 [==============================] - 0s 242us/step\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 16)                2112      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 162,129\n",
      "Trainable params: 162,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 16))\n",
    "model.add(LSTM(16))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(optimizer = 'rmsprop', loss = \"binary_crossentropy\", metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs = 20, batch_size = BATCH_SIZE)\n",
    "results = model.evaluate(x_test, y_test)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6578556467692057, 0.6606666669845581]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'embedding_marix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-d673987f1e71>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0membedding_marix\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'embedding_marix' is not defined"
     ]
    }
   ],
   "source": [
    "model.layers[0].set_weights([embedding_marix])\n",
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer= 'rmsprop', loss = 'binary_crossentropy', metrics = ['accuaracy'])\n",
    "history = model.fit(x_train, y_train, 40, batch_size= 32, validation-data = (x_val, y_val))\n",
    "model.save_weights('pre_trained_glove_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "x = np.array(x_vals)\n",
    "random.seed(SEED)\n",
    "random.shuffle(x)\n",
    "test_data = x[:1500]\n",
    "train_data = x[1500:]\n",
    "\n",
    "def vectorize_sequences(sequences, dimension=10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "y = np.asarray(bin_intro_extro).astype('float32')\n",
    "random.seed(SEED)\n",
    "random.shuffle(y)\n",
    "y_test = y[:1500]\n",
    "y_train = y[1500:]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x_val = x_train[:1500]\n",
    "x_partial_train = x_train[1500:]\n",
    "\n",
    "y_val = y_train[:1500]\n",
    "y_partial_train = y_train[1500:]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# FEED FORWARD MODEL\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "import tensorflow as tf\n",
    "#kernel_regularizer=regularizers.l1_l2(l1=0.001, l2=0.001)\n",
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(LAYER1_SIZE, activation=LAYER1_ACTIVATION, input_shape = (LAYER1_INPUT_DIMENSION,)))\n",
    "    model.add(layers.Dense(LAYER2_SIZE, activation=LAYER2_ACTIVATION))\n",
    "    model.add(layers.Dense(1, activation=LAYER3_ACTIVATION))\n",
    "    model.compile(optimizer = OPTIMIZER, loss = LOSS, metrics = [METRICS])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# LSTM MODEL\n",
    "\n",
    "from keras import models \n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "def build_model():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Embedding(10000, 32))\n",
    "    model.add(layers.LSTM(32))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "with tf.device('/gpu:0'):\n",
    "    history = model.fit(x_train, y_train, epochs = EPOCHS, batch_size = BATCH_SIZE)\n",
    "    results = model.evaluate(x_test, y_test)\n",
    "str(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "log_file = open('results.log', 'a+')\n",
    "\n",
    "stat = \"=================================\\n\" + \\\n",
    "str(now.month) + \"/\" + str(now.day) + \"/\" + str(now.year) + \" \" + \\\n",
    "str(now.hour) + \":\" + str(now.minute) + \":\" + str(now.second) + \"\\n\" + \\\n",
    "\"\\nSEED:\\t\" + str(SEED) +\" \\n\" + \\\n",
    "\"Layer1:\\t\" + str(LAYER1_PARAMS) +\" \\n\" + \\\n",
    "\"Layer2:\\t\" + str(LAYER2_PARAMS) +\" \\n\" + \\\n",
    "\"Layer3:\\t\" + str(LAYER3_PARAMS) +\" \\n\" + \\\n",
    "\"Generalizers:\\t\" + \"\\n\" + \\\n",
    "\"Compilation:\\t\" + str(COMPILATION_PARAMS) +\" \\n\" + \\\n",
    "\"Training: \" + \"EPOCHS \" + str(EPOCHS) + \" | \" + \"BATCH SIZE \" + str(BATCH_SIZE) + \"\\n\" + \\\n",
    "\"\\tRESULTS:\\t\" + \"LOSS:\" + str(results[0]) +  \" | \"+\"ACCURACY:\" + (str(results[1])) + \"\\n\" + \\\n",
    "\"\\n\"\n",
    "\n",
    "print(stat)\n",
    "log_file.write(stat)\n",
    "log_file.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = build_model()\n",
    "with tf.device('/gpu:0'):\n",
    "    history = model.fit(x_partial_train, y_partial_train, epochs = 40, batch_size = 1024, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_dict = history.history\n",
    "train_loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "train_acc = history_dict['acc']\n",
    "val_acc   = history_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(history_dict['acc']) + 1)\n",
    "\n",
    "plt.plot(epochs, train_loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "plt.plot(epochs, train_acc, 'bo', label='Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation Accuracy')\n",
    "plt.title('Training and validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#K-Fold Validation\n",
    "k = 4\n",
    "\n",
    "# // is floor div operator\n",
    "num_val_samples = len(x_train) // k\n",
    "\n",
    "k_val_loss = []\n",
    "k_val_acc = []\n",
    "\n",
    "for num_epochs in range(1, 20):\n",
    "    print(\"Epoch:\",num_epochs)\n",
    "    loss = []\n",
    "    acc = []\n",
    "    random.shuffle(x_train)\n",
    "    random.shuffle(y_train)\n",
    "    for i in range(k):\n",
    "        val_data = x_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "        val_targets = y_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "        partial_train_data = np.concatenate(\n",
    "            [x_train[:i * num_val_samples],\n",
    "             x_train[(i + 1) * num_val_samples:]],\n",
    "            axis=0)\n",
    "        partial_train_targets = np.concatenate(\n",
    "            [y_train[:i * num_val_samples],\n",
    "             y_train[(i + 1) * num_val_samples:]],\n",
    "            axis=0)\n",
    "\n",
    "        model = build_model()\n",
    "        with tf.device('/gpu:0'):\n",
    "            model.fit(partial_train_data, partial_train_targets,\n",
    "                epochs=num_epochs, batch_size=7000,verbose = 0)\n",
    "\n",
    "            val_loss, val_acc = model.evaluate(val_data, val_targets, verbose = 0)\n",
    "\n",
    "        loss.append(val_loss)\n",
    "        acc.append(val_acc)\n",
    "    k_val_loss.append(np.mean(loss))\n",
    "    k_val_acc.append(np.mean(acc))\n",
    "    print(str(k_val_loss[num_epochs - 1]), str(k_val_acc[num_epochs - 1]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = range(1, 12)\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "plt.plot(epochs, k_val_acc, 'b', label='Validation Accuracy')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "\n",
    "plt.plot(epochs, k_val_loss, 'bo', label='Validation loss')\n",
    "plt.title('Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
